SBATCH_INFO: Printing diagnostics (visible devices and nvidia-smi)...
0
Fri Apr 11 11:25:50 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          Off |   00000000:98:00.0 Off |                    0 |
| N/A   51C    P0             50W /  300W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
SBATCH_INFO: Running character recognition training...
model         : cnn_base
batch_size    : 64
epochs        : 15
learning_rate : 0.001
val_split     : 0.2
use_colour    : False
image_size    : [28, 28]
train_path    : dataset/train_letter
test_path     : dataset/test
========== TRAIN.PY START ==========
Using device: cuda
Loading data loaders...
Initializing cnn_base model...
Begin training...
Epoch 1/15 | Batch 0/2117 | Loss: 3.5913 | Acc: 0.00%
Epoch 1/15 | Batch 250/2117 | Loss: 3.4285 | Acc: 4.25%
Epoch 1/15 | Batch 500/2117 | Loss: 2.7031 | Acc: 9.88%
Epoch 1/15 | Batch 750/2117 | Loss: 2.3232 | Acc: 16.57%
Epoch 1/15 | Batch 1000/2117 | Loss: 2.0958 | Acc: 21.78%
Epoch 1/15 | Batch 1250/2117 | Loss: 1.9704 | Acc: 26.16%
Epoch 1/15 | Batch 1500/2117 | Loss: 1.8724 | Acc: 29.65%
Epoch 1/15 | Batch 1750/2117 | Loss: 1.9628 | Acc: 32.55%
Epoch 1/15 | Batch 2000/2117 | Loss: 1.9978 | Acc: 35.03%
Epoch [1/15], Avg Loss: 2.3531, Avg Val Loss: 1.6911, Accuracy: 36.01%
Checkpoint saved at trained_models/cnn_base_checkpoint.pt
Epoch 2/15 | Batch 0/2117 | Loss: 1.7124 | Acc: 57.81%
Epoch 2/15 | Batch 250/2117 | Loss: 1.4542 | Acc: 55.52%
Epoch 2/15 | Batch 500/2117 | Loss: 1.8085 | Acc: 55.88%
Epoch 2/15 | Batch 750/2117 | Loss: 1.2196 | Acc: 56.66%
Epoch 2/15 | Batch 1000/2117 | Loss: 1.4561 | Acc: 57.40%
Epoch 2/15 | Batch 1250/2117 | Loss: 1.3961 | Acc: 58.00%
Epoch 2/15 | Batch 1500/2117 | Loss: 1.3249 | Acc: 58.49%
Epoch 2/15 | Batch 1750/2117 | Loss: 1.3873 | Acc: 59.11%
Epoch 2/15 | Batch 2000/2117 | Loss: 1.1191 | Acc: 59.68%
Epoch [2/15], Avg Loss: 1.4687, Avg Val Loss: 1.2971, Accuracy: 59.93%
Checkpoint saved at trained_models/cnn_base_checkpoint.pt
Epoch 3/15 | Batch 0/2117 | Loss: 1.5157 | Acc: 60.94%
Epoch 3/15 | Batch 250/2117 | Loss: 1.6008 | Acc: 64.62%
Epoch 3/15 | Batch 500/2117 | Loss: 1.5152 | Acc: 65.09%
Epoch 3/15 | Batch 750/2117 | Loss: 0.7943 | Acc: 65.44%
Epoch 3/15 | Batch 1000/2117 | Loss: 1.1678 | Acc: 65.71%
Epoch 3/15 | Batch 1250/2117 | Loss: 1.2986 | Acc: 66.06%
Epoch 3/15 | Batch 1500/2117 | Loss: 1.1187 | Acc: 66.39%
Epoch 3/15 | Batch 1750/2117 | Loss: 1.0762 | Acc: 66.73%
Epoch 3/15 | Batch 2000/2117 | Loss: 1.0699 | Acc: 66.94%
Epoch [3/15], Avg Loss: 1.1825, Avg Val Loss: 1.1842, Accuracy: 67.06%
Checkpoint saved at trained_models/cnn_base_checkpoint.pt
Epoch 4/15 | Batch 0/2117 | Loss: 0.7230 | Acc: 79.69%
Epoch 4/15 | Batch 250/2117 | Loss: 1.0185 | Acc: 69.83%
Epoch 4/15 | Batch 500/2117 | Loss: 1.2126 | Acc: 69.73%
Epoch 4/15 | Batch 750/2117 | Loss: 1.2164 | Acc: 69.86%
Epoch 4/15 | Batch 1000/2117 | Loss: 1.3005 | Acc: 70.10%
Epoch 4/15 | Batch 1250/2117 | Loss: 0.9490 | Acc: 70.28%
Epoch 4/15 | Batch 1500/2117 | Loss: 0.8934 | Acc: 70.53%
Epoch 4/15 | Batch 1750/2117 | Loss: 1.1751 | Acc: 70.74%
Epoch 4/15 | Batch 2000/2117 | Loss: 1.1525 | Acc: 70.96%
Epoch [4/15], Avg Loss: 1.0301, Avg Val Loss: 1.0217, Accuracy: 71.05%
Checkpoint saved at trained_models/cnn_base_checkpoint.pt
Epoch 5/15 | Batch 0/2117 | Loss: 0.8644 | Acc: 76.56%
Epoch 5/15 | Batch 250/2117 | Loss: 0.9467 | Acc: 72.68%
Epoch 5/15 | Batch 500/2117 | Loss: 1.0460 | Acc: 72.52%
Epoch 5/15 | Batch 750/2117 | Loss: 1.4708 | Acc: 72.80%
Epoch 5/15 | Batch 1000/2117 | Loss: 1.4494 | Acc: 72.94%
Epoch 5/15 | Batch 1250/2117 | Loss: 0.9251 | Acc: 73.10%
Epoch 5/15 | Batch 1500/2117 | Loss: 0.9017 | Acc: 73.25%
Epoch 5/15 | Batch 1750/2117 | Loss: 0.8472 | Acc: 73.29%
Epoch 5/15 | Batch 2000/2117 | Loss: 1.1077 | Acc: 73.43%
Epoch [5/15], Avg Loss: 0.9340, Avg Val Loss: 0.9253, Accuracy: 73.45%
Checkpoint saved at trained_models/cnn_base_checkpoint.pt
Epoch 6/15 | Batch 0/2117 | Loss: 0.9248 | Acc: 78.12%
Epoch 6/15 | Batch 250/2117 | Loss: 1.0133 | Acc: 74.74%
Epoch 6/15 | Batch 500/2117 | Loss: 0.7688 | Acc: 74.72%
Epoch 6/15 | Batch 750/2117 | Loss: 0.5571 | Acc: 74.89%
Epoch 6/15 | Batch 1000/2117 | Loss: 0.6707 | Acc: 74.89%
Epoch 6/15 | Batch 1250/2117 | Loss: 0.9119 | Acc: 75.09%
Epoch 6/15 | Batch 1500/2117 | Loss: 0.6521 | Acc: 75.16%
Epoch 6/15 | Batch 1750/2117 | Loss: 0.8123 | Acc: 75.29%
Epoch 6/15 | Batch 2000/2117 | Loss: 0.6853 | Acc: 75.32%
Epoch [6/15], Avg Loss: 0.8639, Avg Val Loss: 0.8698, Accuracy: 75.34%
Checkpoint saved at trained_models/cnn_base_checkpoint.pt
Epoch 7/15 | Batch 0/2117 | Loss: 1.0818 | Acc: 78.12%
Epoch 7/15 | Batch 250/2117 | Loss: 0.8102 | Acc: 76.28%
Epoch 7/15 | Batch 500/2117 | Loss: 0.6433 | Acc: 76.49%
Epoch 7/15 | Batch 750/2117 | Loss: 0.8055 | Acc: 76.74%
Epoch 7/15 | Batch 1000/2117 | Loss: 0.8335 | Acc: 76.79%
Epoch 7/15 | Batch 1250/2117 | Loss: 1.1304 | Acc: 76.64%
Epoch 7/15 | Batch 1500/2117 | Loss: 0.6693 | Acc: 76.58%
Epoch 7/15 | Batch 1750/2117 | Loss: 0.9431 | Acc: 76.57%
Epoch 7/15 | Batch 2000/2117 | Loss: 0.5786 | Acc: 76.63%
Epoch [7/15], Avg Loss: 0.8100, Avg Val Loss: 0.8371, Accuracy: 76.69%
Checkpoint saved at trained_models/cnn_base_checkpoint.pt
Epoch 8/15 | Batch 0/2117 | Loss: 1.2578 | Acc: 64.06%
Epoch 8/15 | Batch 250/2117 | Loss: 0.6684 | Acc: 77.68%
Epoch 8/15 | Batch 500/2117 | Loss: 0.7259 | Acc: 77.74%
Epoch 8/15 | Batch 750/2117 | Loss: 0.8313 | Acc: 77.68%
Epoch 8/15 | Batch 1000/2117 | Loss: 0.7623 | Acc: 77.73%
Epoch 8/15 | Batch 1250/2117 | Loss: 1.1020 | Acc: 77.76%
Epoch 8/15 | Batch 1500/2117 | Loss: 0.7603 | Acc: 77.81%
Epoch 8/15 | Batch 1750/2117 | Loss: 0.6637 | Acc: 77.83%
Epoch 8/15 | Batch 2000/2117 | Loss: 0.8131 | Acc: 77.88%
Epoch [8/15], Avg Loss: 0.7678, Avg Val Loss: 0.7933, Accuracy: 77.87%
Checkpoint saved at trained_models/cnn_base_checkpoint.pt
Epoch 9/15 | Batch 0/2117 | Loss: 0.9013 | Acc: 75.00%
Epoch 9/15 | Batch 250/2117 | Loss: 0.8280 | Acc: 77.88%
Epoch 9/15 | Batch 500/2117 | Loss: 1.3719 | Acc: 78.14%
Epoch 9/15 | Batch 750/2117 | Loss: 0.7268 | Acc: 78.48%
Epoch 9/15 | Batch 1000/2117 | Loss: 0.3899 | Acc: 78.53%
Epoch 9/15 | Batch 1250/2117 | Loss: 0.8873 | Acc: 78.58%
Epoch 9/15 | Batch 1500/2117 | Loss: 0.3965 | Acc: 78.65%
Epoch 9/15 | Batch 1750/2117 | Loss: 0.6946 | Acc: 78.66%
Epoch 9/15 | Batch 2000/2117 | Loss: 0.4524 | Acc: 78.72%
Epoch [9/15], Avg Loss: 0.7329, Avg Val Loss: 0.7950, Accuracy: 78.77%
Checkpoint saved at trained_models/cnn_base_checkpoint.pt
Epoch 10/15 | Batch 0/2117 | Loss: 0.7196 | Acc: 75.00%
Epoch 10/15 | Batch 250/2117 | Loss: 0.8240 | Acc: 79.56%
Epoch 10/15 | Batch 500/2117 | Loss: 0.5918 | Acc: 79.46%
Epoch 10/15 | Batch 750/2117 | Loss: 0.7613 | Acc: 79.54%
Epoch 10/15 | Batch 1000/2117 | Loss: 0.9226 | Acc: 79.45%
Epoch 10/15 | Batch 1250/2117 | Loss: 0.5781 | Acc: 79.45%
Epoch 10/15 | Batch 1500/2117 | Loss: 0.8559 | Acc: 79.51%
Epoch 10/15 | Batch 1750/2117 | Loss: 0.7077 | Acc: 79.51%
Epoch 10/15 | Batch 2000/2117 | Loss: 0.7602 | Acc: 79.59%
Epoch [10/15], Avg Loss: 0.7008, Avg Val Loss: 0.7436, Accuracy: 79.58%
Checkpoint saved at trained_models/cnn_base_checkpoint.pt
Epoch 11/15 | Batch 0/2117 | Loss: 0.5511 | Acc: 79.69%
Epoch 11/15 | Batch 250/2117 | Loss: 0.7566 | Acc: 80.14%
Epoch 11/15 | Batch 500/2117 | Loss: 0.5490 | Acc: 80.17%
Epoch 11/15 | Batch 750/2117 | Loss: 0.4589 | Acc: 80.13%
Epoch 11/15 | Batch 1000/2117 | Loss: 0.5881 | Acc: 80.28%
Epoch 11/15 | Batch 1250/2117 | Loss: 0.6341 | Acc: 80.28%
Epoch 11/15 | Batch 1500/2117 | Loss: 0.5724 | Acc: 80.18%
Epoch 11/15 | Batch 1750/2117 | Loss: 0.5804 | Acc: 80.15%
Epoch 11/15 | Batch 2000/2117 | Loss: 0.8796 | Acc: 80.21%
Epoch [11/15], Avg Loss: 0.6751, Avg Val Loss: 0.7261, Accuracy: 80.22%
Checkpoint saved at trained_models/cnn_base_checkpoint.pt
Epoch 12/15 | Batch 0/2117 | Loss: 0.6092 | Acc: 81.25%
Epoch 12/15 | Batch 250/2117 | Loss: 0.6771 | Acc: 80.63%
Epoch 12/15 | Batch 500/2117 | Loss: 0.4766 | Acc: 80.85%
Epoch 12/15 | Batch 750/2117 | Loss: 0.7369 | Acc: 80.76%
Epoch 12/15 | Batch 1000/2117 | Loss: 0.3975 | Acc: 80.78%
Epoch 12/15 | Batch 1250/2117 | Loss: 0.7084 | Acc: 80.83%
Epoch 12/15 | Batch 1500/2117 | Loss: 0.6412 | Acc: 80.87%
Epoch 12/15 | Batch 1750/2117 | Loss: 0.6314 | Acc: 80.80%
Epoch 12/15 | Batch 2000/2117 | Loss: 0.4813 | Acc: 80.80%
Epoch [12/15], Avg Loss: 0.6499, Avg Val Loss: 0.7122, Accuracy: 80.82%
Checkpoint saved at trained_models/cnn_base_checkpoint.pt
Epoch 13/15 | Batch 0/2117 | Loss: 0.4111 | Acc: 85.94%
Epoch 13/15 | Batch 250/2117 | Loss: 0.6016 | Acc: 81.09%
Epoch 13/15 | Batch 500/2117 | Loss: 0.7368 | Acc: 81.49%
Epoch 13/15 | Batch 750/2117 | Loss: 0.5969 | Acc: 81.54%
Epoch 13/15 | Batch 1000/2117 | Loss: 0.5196 | Acc: 81.49%
Epoch 13/15 | Batch 1250/2117 | Loss: 0.6339 | Acc: 81.35%
Epoch 13/15 | Batch 1500/2117 | Loss: 0.5310 | Acc: 81.46%
Epoch 13/15 | Batch 1750/2117 | Loss: 0.5479 | Acc: 81.35%
Epoch 13/15 | Batch 2000/2117 | Loss: 0.5982 | Acc: 81.38%
Epoch [13/15], Avg Loss: 0.6307, Avg Val Loss: 0.7199, Accuracy: 81.37%
Checkpoint saved at trained_models/cnn_base_checkpoint.pt
Epoch 14/15 | Batch 0/2117 | Loss: 1.0129 | Acc: 79.69%
Epoch 14/15 | Batch 250/2117 | Loss: 0.6224 | Acc: 81.58%
Epoch 14/15 | Batch 500/2117 | Loss: 0.6247 | Acc: 81.50%
Epoch 14/15 | Batch 750/2117 | Loss: 0.7451 | Acc: 81.57%
Epoch 14/15 | Batch 1000/2117 | Loss: 0.5785 | Acc: 81.59%
Epoch 14/15 | Batch 1250/2117 | Loss: 0.6698 | Acc: 81.60%
Epoch 14/15 | Batch 1500/2117 | Loss: 0.3478 | Acc: 81.65%
Epoch 14/15 | Batch 1750/2117 | Loss: 0.4467 | Acc: 81.64%
Epoch 14/15 | Batch 2000/2117 | Loss: 0.5881 | Acc: 81.71%
Epoch [14/15], Avg Loss: 0.6128, Avg Val Loss: 0.6820, Accuracy: 81.71%
Checkpoint saved at trained_models/cnn_base_checkpoint.pt
Epoch 15/15 | Batch 0/2117 | Loss: 0.8256 | Acc: 81.25%
Epoch 15/15 | Batch 250/2117 | Loss: 0.5184 | Acc: 81.75%
Epoch 15/15 | Batch 500/2117 | Loss: 0.8217 | Acc: 82.02%
Epoch 15/15 | Batch 750/2117 | Loss: 0.5232 | Acc: 82.19%
Epoch 15/15 | Batch 1000/2117 | Loss: 0.7037 | Acc: 82.29%
Epoch 15/15 | Batch 1250/2117 | Loss: 0.7164 | Acc: 82.23%
Epoch 15/15 | Batch 1500/2117 | Loss: 0.4371 | Acc: 82.33%
Epoch 15/15 | Batch 1750/2117 | Loss: 0.4318 | Acc: 82.26%
Epoch 15/15 | Batch 2000/2117 | Loss: 0.3699 | Acc: 82.25%
Epoch [15/15], Avg Loss: 0.5953, Avg Val Loss: 0.6632, Accuracy: 82.27%
Checkpoint saved at trained_models/cnn_base_checkpoint.pt
Training complete!
Model saved to trained_models/cnn_base.pt
Saved learning curve as learning_curves/cnn_base.png
Checkpoint deleted after successful training.
SBATCH_INFO: Running character recognition evaluation...
========== EVALUATE.PY START ==========
Model loaded successfully from 'trained_models/cnn_base.pt'
Loading test dataset...
Evaluating model...
Batch 0 | Word Acc: 0.00% | Char Acc: 0.00%
Batch 50 | Word Acc: 37.25% | Char Acc: 73.95%
Batch 100 | Word Acc: 37.62% | Char Acc: 73.55%
Batch 150 | Word Acc: 37.09% | Char Acc: 75.22%
Batch 200 | Word Acc: 35.82% | Char Acc: 74.75%
Batch 250 | Word Acc: 35.86% | Char Acc: 75.13%
Batch 300 | Word Acc: 34.55% | Char Acc: 75.19%
Batch 350 | Word Acc: 35.33% | Char Acc: 75.69%
Batch 400 | Word Acc: 35.41% | Char Acc: 75.77%
Batch 450 | Word Acc: 36.81% | Char Acc: 75.87%
Batch 500 | Word Acc: 38.12% | Char Acc: 76.52%
Batch 550 | Word Acc: 37.93% | Char Acc: 76.72%
Batch 600 | Word Acc: 38.10% | Char Acc: 76.34%
Batch 650 | Word Acc: 37.48% | Char Acc: 76.20%
Batch 700 | Word Acc: 37.52% | Char Acc: 76.13%
Batch 750 | Word Acc: 37.55% | Char Acc: 76.25%
Batch 800 | Word Acc: 36.58% | Char Acc: 75.95%
Batch 850 | Word Acc: 36.43% | Char Acc: 75.83%
Batch 900 | Word Acc: 36.07% | Char Acc: 75.96%
Batch 950 | Word Acc: 36.38% | Char Acc: 75.97%
Batch 1000 | Word Acc: 36.76% | Char Acc: 76.07%
Batch 1050 | Word Acc: 37.20% | Char Acc: 76.43%
Batch 1100 | Word Acc: 37.24% | Char Acc: 76.50%
Batch 1150 | Word Acc: 37.19% | Char Acc: 76.57%
Batch 1200 | Word Acc: 36.89% | Char Acc: 76.69%
Batch 1250 | Word Acc: 36.85% | Char Acc: 76.75%
Batch 1300 | Word Acc: 36.89% | Char Acc: 76.63%
Batch 1350 | Word Acc: 36.94% | Char Acc: 76.63%
Batch 1400 | Word Acc: 37.04% | Char Acc: 76.76%
Batch 1450 | Word Acc: 36.94% | Char Acc: 76.85%
Batch 1500 | Word Acc: 37.38% | Char Acc: 76.75%
Batch 1550 | Word Acc: 37.33% | Char Acc: 76.57%
Batch 1600 | Word Acc: 37.91% | Char Acc: 76.73%
Batch 1650 | Word Acc: 37.73% | Char Acc: 76.72%
Batch 1700 | Word Acc: 37.74% | Char Acc: 76.62%
Batch 1750 | Word Acc: 38.15% | Char Acc: 76.73%
Batch 1800 | Word Acc: 38.15% | Char Acc: 76.79%
Batch 1850 | Word Acc: 38.14% | Char Acc: 76.76%
Batch 1900 | Word Acc: 37.93% | Char Acc: 76.85%
Batch 1950 | Word Acc: 37.67% | Char Acc: 76.77%
Word Accuracy: 37.75%
Character Accuracy: 76.89%
Character-level Macro Precision: 78.51%
Character-level Macro Recall: 77.96%
Character-level Macro F1 Score: 78.03%
