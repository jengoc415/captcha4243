SBATCH_INFO: Printing diagnostics (visible devices and nvidia-smi)...
0
Fri Apr 11 17:25:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          Off |   00000000:98:00.0 Off |                    0 |
| N/A   63C    P0             90W /  300W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
SBATCH_INFO: Running character recognition training...
model         : rnn_base
batch_size    : 16
epochs        : 25
learning_rate : 0.001
val_split     : 0.2
use_colour    : False
image_size    : [200, 50]
train_path    : dataset/train
test_path     : dataset/test
========== TRAIN.PY START ==========
Using device: cuda
Loading data loaders...
Initializing rnn_base model...
Begin training...
Epoch 1/25 | Batch 0/401 | Loss: 24.9438
Epoch 1/25 | Batch 250/401 | Loss: 3.9499
Epoch [1/25], Avg Loss: 4.1612, Avg Val Loss: 3.9417
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 2/25 | Batch 0/401 | Loss: 3.9501
Epoch 2/25 | Batch 250/401 | Loss: 3.9459
Epoch [2/25], Avg Loss: 3.9405, Avg Val Loss: 3.9403
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 3/25 | Batch 0/401 | Loss: 3.9497
Epoch 3/25 | Batch 250/401 | Loss: 3.9268
Epoch [3/25], Avg Loss: 3.9392, Avg Val Loss: 3.9426
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 4/25 | Batch 0/401 | Loss: 3.9654
Epoch 4/25 | Batch 250/401 | Loss: 3.9359
Epoch [4/25], Avg Loss: 3.9317, Avg Val Loss: 3.9321
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 5/25 | Batch 0/401 | Loss: 3.9215
Epoch 5/25 | Batch 250/401 | Loss: 3.8783
Epoch [5/25], Avg Loss: 3.8931, Avg Val Loss: 3.8510
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 6/25 | Batch 0/401 | Loss: 3.8747
Epoch 6/25 | Batch 250/401 | Loss: 3.8695
Epoch [6/25], Avg Loss: 3.8250, Avg Val Loss: 3.7688
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 7/25 | Batch 0/401 | Loss: 3.7651
Epoch 7/25 | Batch 250/401 | Loss: 3.6886
Epoch [7/25], Avg Loss: 3.7132, Avg Val Loss: 3.6360
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 8/25 | Batch 0/401 | Loss: 3.5472
Epoch 8/25 | Batch 250/401 | Loss: 3.5904
Epoch [8/25], Avg Loss: 3.5541, Avg Val Loss: 3.4880
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 9/25 | Batch 0/401 | Loss: 3.4152
Epoch 9/25 | Batch 250/401 | Loss: 3.1389
Epoch [9/25], Avg Loss: 3.3632, Avg Val Loss: 3.2954
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 10/25 | Batch 0/401 | Loss: 3.0226
Epoch 10/25 | Batch 250/401 | Loss: 2.4003
Epoch [10/25], Avg Loss: 2.4144, Avg Val Loss: 1.8409
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 11/25 | Batch 0/401 | Loss: 2.0438
Epoch 11/25 | Batch 250/401 | Loss: 1.2339
Epoch [11/25], Avg Loss: 1.5513, Avg Val Loss: 1.4694
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 12/25 | Batch 0/401 | Loss: 1.2736
Epoch 12/25 | Batch 250/401 | Loss: 0.9696
Epoch [12/25], Avg Loss: 1.2663, Avg Val Loss: 1.3164
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 13/25 | Batch 0/401 | Loss: 1.0486
Epoch 13/25 | Batch 250/401 | Loss: 1.1753
Epoch [13/25], Avg Loss: 1.0902, Avg Val Loss: 1.2326
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 14/25 | Batch 0/401 | Loss: 1.1127
Epoch 14/25 | Batch 250/401 | Loss: 0.7579
Epoch [14/25], Avg Loss: 0.9298, Avg Val Loss: 1.1583
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 15/25 | Batch 0/401 | Loss: 0.9176
Epoch 15/25 | Batch 250/401 | Loss: 0.7250
Epoch [15/25], Avg Loss: 0.8195, Avg Val Loss: 1.1059
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 16/25 | Batch 0/401 | Loss: 0.6391
Epoch 16/25 | Batch 250/401 | Loss: 0.4609
Epoch [16/25], Avg Loss: 0.7131, Avg Val Loss: 1.0700
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 17/25 | Batch 0/401 | Loss: 0.6959
Epoch 17/25 | Batch 250/401 | Loss: 0.4952
Epoch [17/25], Avg Loss: 0.6211, Avg Val Loss: 1.0505
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 18/25 | Batch 0/401 | Loss: 0.2828
Epoch 18/25 | Batch 250/401 | Loss: 0.9404
Epoch [18/25], Avg Loss: 0.5413, Avg Val Loss: 1.0418
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 19/25 | Batch 0/401 | Loss: 0.3997
Epoch 19/25 | Batch 250/401 | Loss: 0.6583
Epoch [19/25], Avg Loss: 0.4739, Avg Val Loss: 1.0652
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 20/25 | Batch 0/401 | Loss: 0.4327
Epoch 20/25 | Batch 250/401 | Loss: 0.8288
Epoch [20/25], Avg Loss: 0.4134, Avg Val Loss: 1.0724
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 21/25 | Batch 0/401 | Loss: 0.2288
Epoch 21/25 | Batch 250/401 | Loss: 0.4289
Epoch [21/25], Avg Loss: 0.3534, Avg Val Loss: 1.0894
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 22/25 | Batch 0/401 | Loss: 0.2927
Epoch 22/25 | Batch 250/401 | Loss: 0.1994
Epoch [22/25], Avg Loss: 0.3022, Avg Val Loss: 1.1206
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 23/25 | Batch 0/401 | Loss: 0.2239
Epoch 23/25 | Batch 250/401 | Loss: 0.1276
Epoch [23/25], Avg Loss: 0.2501, Avg Val Loss: 1.1195
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 24/25 | Batch 0/401 | Loss: 0.2025
Epoch 24/25 | Batch 250/401 | Loss: 0.1805
Epoch [24/25], Avg Loss: 0.2179, Avg Val Loss: 1.1715
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Epoch 25/25 | Batch 0/401 | Loss: 0.1903
Epoch 25/25 | Batch 250/401 | Loss: 0.2406
Epoch [25/25], Avg Loss: 0.2027, Avg Val Loss: 1.1462
Checkpoint saved at trained_models/rnn_base_checkpoint.pt
Training complete!
Model saved to trained_models/rnn_base.pt
Saved learning curve as learning_curves/rnn_base.png
Checkpoint deleted after successful training.
SBATCH_INFO: Running character recognition evaluation...
========== EVALUATE.PY START ==========
Model loaded successfully from 'trained_models/rnn_base.pt'
Loading test dataset...
Evaluating model...
Batch 0 | Word Acc: 0.00% | Char Acc: 40.00%
Batch 50 | Word Acc: 29.41% | Char Acc: 72.03%
Batch 100 | Word Acc: 29.70% | Char Acc: 71.94%
Batch 150 | Word Acc: 29.80% | Char Acc: 73.78%
Batch 200 | Word Acc: 27.36% | Char Acc: 71.85%
Batch 250 | Word Acc: 28.29% | Char Acc: 72.35%
Batch 300 | Word Acc: 30.23% | Char Acc: 72.83%
Batch 350 | Word Acc: 31.91% | Char Acc: 73.69%
Batch 400 | Word Acc: 30.67% | Char Acc: 73.15%
Batch 450 | Word Acc: 30.60% | Char Acc: 72.98%
Batch 500 | Word Acc: 30.34% | Char Acc: 72.85%
Batch 550 | Word Acc: 30.13% | Char Acc: 73.25%
Batch 600 | Word Acc: 30.78% | Char Acc: 73.03%
Batch 650 | Word Acc: 30.72% | Char Acc: 72.40%
Batch 700 | Word Acc: 30.39% | Char Acc: 72.32%
Batch 750 | Word Acc: 30.23% | Char Acc: 72.47%
Batch 800 | Word Acc: 29.46% | Char Acc: 72.08%
Batch 850 | Word Acc: 29.38% | Char Acc: 71.99%
Batch 900 | Word Acc: 29.19% | Char Acc: 71.90%
Batch 950 | Word Acc: 28.92% | Char Acc: 71.78%
Batch 1000 | Word Acc: 29.17% | Char Acc: 71.76%
Batch 1050 | Word Acc: 29.69% | Char Acc: 72.16%
Batch 1100 | Word Acc: 29.79% | Char Acc: 72.28%
Batch 1150 | Word Acc: 30.23% | Char Acc: 72.64%
Batch 1200 | Word Acc: 30.14% | Char Acc: 72.58%
Batch 1250 | Word Acc: 30.46% | Char Acc: 72.65%
Batch 1300 | Word Acc: 30.21% | Char Acc: 72.30%
Batch 1350 | Word Acc: 29.83% | Char Acc: 72.14%
Batch 1400 | Word Acc: 30.12% | Char Acc: 72.24%
Batch 1450 | Word Acc: 30.19% | Char Acc: 72.29%
Batch 1500 | Word Acc: 30.58% | Char Acc: 72.39%
Batch 1550 | Word Acc: 30.69% | Char Acc: 72.27%
Batch 1600 | Word Acc: 30.73% | Char Acc: 72.27%
Batch 1650 | Word Acc: 30.53% | Char Acc: 72.21%
Batch 1700 | Word Acc: 30.45% | Char Acc: 72.00%
Batch 1750 | Word Acc: 30.61% | Char Acc: 72.10%
Batch 1800 | Word Acc: 30.76% | Char Acc: 72.21%
Batch 1850 | Word Acc: 30.74% | Char Acc: 72.19%
Batch 1900 | Word Acc: 30.62% | Char Acc: 72.13%
Batch 1950 | Word Acc: 30.39% | Char Acc: 72.13%
Word Accuracy: 30.30%
Character Accuracy: 72.20%
Character-level Macro Precision: 73.39%
Character-level Macro Recall: 73.00%
Character-level Macro F1 Score: 73.09%
